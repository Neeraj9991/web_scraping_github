{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top repositories for Github Topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction :\n",
    "This Python notebook performs web scraping to extract information about the top repositories for different topics from GitHub. It utilizes the requests library to fetch HTML content from web pages and Beautiful Soup to parse and navigate the HTML data. The extracted data is stored in a structured format using the Pandas library, and the results are saved as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement : \n",
    "The goal of this project is to perform web scraping on GitHub topics and extract information about the top repositories for each topic. The script should navigate through the GitHub website, collect data about different topics, and then scrape data from the corresponding topic pages to retrieve details about the top repositories. The extracted data should be organized and saved in a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies :\n",
    "1. Python 3.x\n",
    "2. `requests` library: To send HTTP requests and fetch web page data.\n",
    "3. `BeautifulSoup` library: To parse and navigate HTML data.\n",
    "4. `Pandas` library: To store and manipulate the extracted data in a tabular format.\n",
    "5. `os` module: To handle file and directory operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:\n",
    "\n",
    "This web scraping project can be divided into two main parts: The first part focuses on extracting the various topics listed on GitHub. The second part deals with obtaining detailed information about the repositories within each of the extracted topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://github.com\"\n",
    "topics_url = base_url + \"/topics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting Topics Page from GitHub:\n",
    "\n",
    "- `get_topics_page(topics_url)` : Fetches and returns the BeautifulSoup object for the GitHub topics page.\n",
    "- `get_topic_details(doc)`: Extracts details about different topics from the topics page and returns a dictionary containing the title, description, and URL of each topic.\n",
    "- `scrape_topics()`: Fetches the GitHub topics page, extracts topic details, and returns a Pandas DataFrame containing the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_page(topics_url):\n",
    "    \n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to Load Page {}'.format(topics_url))\n",
    "    \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(doc):\n",
    "\n",
    "    title = []\n",
    "    description = []\n",
    "    url = []\n",
    "\n",
    "    # Iterate over each <div> element and extract title, description, and URL\n",
    "    for item in doc:\n",
    "        title.append(item.find('p', class_='f3 lh-condensed mb-0 mt-1 Link--primary').text.strip())\n",
    "        \n",
    "        description.append(item.find('p', class_='f5 color-fg-muted mb-0 mt-1').text.strip())\n",
    "        \n",
    "        url.append(base_url + item.find('a', class_='no-underline flex-1 d-flex flex-column')['href'])\n",
    "\n",
    "    topics_dict = {'title': title, 'description': description, 'url': url}\n",
    "\n",
    "    return topics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "\n",
    "    doc = get_topics_page(topics_url)\n",
    "    \n",
    "    div_tags = doc.find_all('div', class_='py-4 border-bottom d-flex flex-justify-between')\n",
    "\n",
    "    topics_df = pd.DataFrame(get_topic_details(div_tags))\n",
    "\n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Getting Repositories from Links Extracted from the Above Process:\n",
    "\n",
    "- `get_repo_info(h3_tags, star_tags)`: Extracts repository information, including username, repository name, star count, and repository URL, from the given HTML tags.\n",
    "- `get_topic_repos(topic_doc)`: Extracts information about top repositories for a specific topic and returns a Pandas DataFrame with the extracted data.\n",
    "- `scrape_topics_repos()`: Scrapes repositories for all topics and saves the combined data to a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tags, star_tags):\n",
    "    a_tags = h3_tags.find_all('a')\n",
    "\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "\n",
    "    stars = star_tags.text.strip()\n",
    "\n",
    "    star_count = 0\n",
    "    if stars[-1] == 'k':\n",
    "        star_count = int(float(stars[:-1]) * 1000)\n",
    "\n",
    "    return username, repo_name, star_count, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \n",
    "    h3_tags = topic_doc.find_all('h3', {\"class\":\"f3 color-fg-muted text-normal lh-condensed\"})\n",
    "\n",
    "    star_tags = topic_doc.find_all('span', {\"class\":\"Counter js-social-count\"})\n",
    "\n",
    "    topic_repos_dict = {\n",
    "    'username': [],\n",
    "    'repo_name': [],\n",
    "    'stars': [],\n",
    "    'repo_url': [],\n",
    "    }\n",
    "\n",
    "    for i in range(len(h3_tags)):\n",
    "        repo_info = get_repo_info(h3_tags[i], star_tags[i])\n",
    "\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "\n",
    "    return pd.DataFrame(topic_repos_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    topics_df = scrape_topics()\n",
    "    combined_df = pd.DataFrame()  # Initialize an empty DataFrame to combine all repositories\n",
    "\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        topic_df = get_topic_repos(get_topics_page(row['url']))\n",
    "        combined_df = pd.concat([combined_df, topic_df], ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a single CSV file\n",
    "    combined_df.to_csv('data/all_repos.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and run this shell to perform webscraping.\n",
    "\n",
    "# scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output:\n",
    "The notebook will create a \"data\" directory if it doesn't exist and save a CSV file named \"all_repos.csv\" inside it. This CSV file will contain information about the top repositories for different topics scraped from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note and Legal Considerations:\n",
    "Please use this notebook responsibly and in compliance with GitHub's terms of service and robots.txt file. Web scraping can potentially put a strain on the website's server, so consider adding reasonable delays between requests. Respect website policies and use web scraping only for legitimate purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "This notebook has provided an overview of the Python script used to perform web scraping on GitHub topics. By running this notebook, you can extract valuable data about top repositories for different topics on GitHub and store it in a structured format for further analysis. Remember to adhere to ethical web scraping practices and respect website policies during the scraping process. Happy coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference for future\n",
    "\n",
    "Summary of what we did \n",
    "- Developed a Python script in the form of a Jupyter notebook for web scraping on GitHub topics.\n",
    "- Extracted information about the top repositories for different topics and stored data in CSV files.\n",
    "- Divided the project into two parts: extraction of GitHub topics and retrieval of detailed repository information.\n",
    "\n",
    "Links to useful references\n",
    "- BeautifulSoup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- Pandas Documentation: https://pandas.pydata.org/docs/user_guide/index.html#user-guide\n",
    "- GitHub API Documentation: https://developer.github.com/v3/\n",
    "\n",
    "Ideas for future work \n",
    "- Allow users to input specific topics or filters for customized data extraction.\n",
    "- Error Handling and Robustness : Handle scenarios such as connection timeouts, invalid responses, or unexpected HTML structure gracefully, ensuring the script continues to run smoothly under various conditions.\n",
    "- Keyword Extraction: Extract key topics and programming language used from repository descriptions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
